{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data import and Fits of the SARS-CoV-2 D614G model\n",
    "\n",
    "* Data from UK and NL\n",
    "* Model fits\n",
    "* Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.stats as sts\n",
    "import xml.etree.ElementTree as ET\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import scipy.special\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.transforms import blended_transform_factory\n",
    "\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import sys, importlib\n",
    "sys.path.append(\"..\")\n",
    "from evpytools import evplot\n",
    "from evpytools import pftools\n",
    "from evpytools import auxiliary as aux\n",
    "from evpytools import definitions as defn\n",
    "for mod in [evplot, pftools, aux, defn]:\n",
    "    importlib.reload(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SARS-CoV-2 data\n",
    "Mutant frequencies and incidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/seq_epi-08282020.csv\"\n",
    "#seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/new_mutations/seq501_epi-21122020\"\n",
    "seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/new_mutations/seq_epi-08282020.csv\"\n",
    "## original B117\n",
    "#seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/new_mutations/seq-pl_epi-07012020.csv\"\n",
    "## longer B117\n",
    "#seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/new_mutations/seq-pl_epi-26022021.csv\"\n",
    "## SA variant\n",
    "#seq_epi_data_file = \"/home/chris/Projects/COVID-19/G614D-mutant/new_mutations/seq-pl-sa_epi-07012020.csv\"\n",
    "\n",
    "## specify variants. Mutant and WT (WT is optional):\n",
    "\n",
    "##### D614G options\n",
    "\n",
    "var_keys = [\"seq_G\", \"seq_D\"]\n",
    "aa_pos = 614\n",
    "total_key = \"seq_total\"\n",
    "\n",
    "\n",
    "##### B.1.1.7 options\n",
    "\n",
    "#var_keys = [\"N_mt\"]\n",
    "#aa_pos = 501 ## will be used for file names!!\n",
    "#total_key = \"N\"\n",
    "\n",
    "\n",
    "##### ZA options\n",
    "\n",
    "#var_keys = [\"N_mt\"]\n",
    "#aa_pos = \"ZA\" ## will be used for file names!!\n",
    "#total_key = \"N\"\n",
    "\n",
    "\n",
    "### IMPORT DATA\n",
    "\n",
    "with open(seq_epi_data_file) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data_dicts = [row for row in reader]\n",
    "\n",
    "\n",
    "fields = var_keys + [total_key, \"deaths\", \"cases\", \"recoveries\"]\n",
    "\n",
    "## start counting from Jan-1-2020\n",
    "day0 = datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "print(\"weekday of day zero:\", day0.weekday()) ## monday = 0, sunday = 6\n",
    "\n",
    "for row in data_dicts:\n",
    "    ## convert date to days from first obs\n",
    "    date = datetime.datetime.strptime(row[\"date\"], \"%Y-%m-%d\")\n",
    "    row[\"date\"] = date\n",
    "    row[\"t\"] = (date - day0).days\n",
    "    ## convert counts to int, add censoring info\n",
    "    for k in fields:\n",
    "        sval = row[k]\n",
    "        if sval != 'NA':\n",
    "            row[k] = int(row[k])\n",
    "            row[k + \"_CC\"] = defn.uncensored_code\n",
    "        else:\n",
    "            row[k] = 0\n",
    "            row[k + \"_CC\"] = defn.missing_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#region = \"Netherlands\"\n",
    "region = \"United Kingdom\"\n",
    "#region = \"South Africa\"\n",
    "sel_data_dicts = [copy.copy(row) for row in data_dicts if row[\"country\"] == region]\n",
    "   \n",
    "\n",
    "## check for negative counts...\n",
    "for dd in sel_data_dicts:\n",
    "    if dd[\"deaths\"] < 0:\n",
    "        print(dd[\"t\"], dd[\"deaths\"])\n",
    "    \n",
    "## apply corrections\n",
    "corrections_NL = {\n",
    "    208 : {\"deaths\" : 0},\n",
    "    207 : {\"deaths\" : 1},\n",
    "    223 : {\"deaths\" : 0},\n",
    "    224 : {\"deaths\" : 4},\n",
    "    191 : {\"deaths\" : 0},\n",
    "    195 : {\"deaths\" : 0},\n",
    "    199 : {\"deaths\" : 0},\n",
    "}\n",
    "\n",
    "if region == \"Netherlands\":\n",
    "    for t, cord in corrections_NL.items():\n",
    "        for dd in sel_data_dicts:\n",
    "            if dd[\"t\"] == t:\n",
    "                for k, v in cord.items():\n",
    "                    dd[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## take a look at the data\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14,7), sharex=True)\n",
    "\n",
    "axs[0].plot([row[\"t\"] for row in sel_data_dicts],\n",
    "            [row[\"cases\"] for row in sel_data_dicts], color=\"k\")\n",
    "\n",
    "bx = axs[0].twinx()\n",
    "\n",
    "bx.plot([row[\"t\"] for row in sel_data_dicts],\n",
    "        [row[\"deaths\"] for row in sel_data_dicts], color=\"red\")\n",
    "\n",
    "axs[0].set_ylabel(\"cases\")\n",
    "bx.set_ylabel(\"deaths\", color='red')\n",
    "\n",
    "Fms = [row[var_keys[0]] / seq_total if (seq_total := row[total_key]) > 0 \n",
    "       else np.nan for row in sel_data_dicts]\n",
    "\n",
    "## CIs\n",
    "lFms = [sts.beta.ppf(0.025, row[var_keys[0]]+0.5, row[total_key]-row[var_keys[0]]+0.5) \n",
    "        for row in sel_data_dicts]\n",
    "uFms = [sts.beta.ppf(0.975, row[var_keys[0]]+0.5, row[total_key]-row[var_keys[0]]+0.5) \n",
    "        for row in sel_data_dicts]\n",
    "\n",
    "ts = [row[\"t\"] for row in sel_data_dicts]\n",
    "dates = [day0 + datetime.timedelta(days=t) for t in ts]\n",
    "date_strs = [date.strftime(\"%m-%d-%Y\") for date in dates]\n",
    "\n",
    "axs[1].scatter(ts, Fms, color=\"k\", s=5)\n",
    "\n",
    "for t, l, u in zip(ts, lFms, uFms):\n",
    "    axs[1].plot([t,t], [l,u], color='k', alpha=0.5)\n",
    "    \n",
    "axs[1].set_ylabel(\"fraction mutant ($F_m$)\")\n",
    "\n",
    "axs[0].set_title(region)\n",
    "\n",
    "dt = 14\n",
    "axs[1].set_xticks(ts[::dt])\n",
    "axs[1].set_xticklabels(date_strs[::dt], rotation=45, ha='right')\n",
    "\n",
    "fig.savefig(f\"../data/out/figures/sars-data-{region}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## compute incidence\n",
    "def add_incidence(dds, cumul=True):\n",
    "    for i in range(len(dds)):\n",
    "        if not cumul:\n",
    "            ## just copy cases and death\n",
    "            dds[i][\"cases_incidence\"] = dds[i][\"cases\"]\n",
    "            dds[i][\"deaths_incidence\"] = dds[i][\"deaths\"]\n",
    "        else: \n",
    "            ## convert cumulative incidence to actual daily incidence\n",
    "            if i == 0:\n",
    "                dds[i][\"cases_incidence\"] = 0\n",
    "                dds[i][\"deaths_incidence\"] = 0\n",
    "            else:\n",
    "                c0 = dds[i-1][\"cases\"]\n",
    "                c1 = dds[i][\"cases\"]\n",
    "                dds[i][\"cases_incidence\"] = c1 - c0\n",
    "                d0 = dds[i-1][\"deaths\"]\n",
    "                d1 = dds[i][\"deaths\"]\n",
    "                dds[i][\"deaths_incidence\"] = d1 - d0\n",
    "        \n",
    "#add_incidence(sel_data_dicts) ## for old data\n",
    "add_incidence(sel_data_dicts, cumul=False) ## for new data\n",
    "        \n",
    "## plot incidence\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,5))\n",
    "bx = ax.twinx()\n",
    "\n",
    "ax.plot(ts, [row[\"cases_incidence\"] for row in sel_data_dicts], color='k')\n",
    "bx.plot(ts, [row[\"deaths_incidence\"] for row in sel_data_dicts], color='red')\n",
    "\n",
    "dt = 14\n",
    "ax.set_xticks(ts[::dt])\n",
    "ax.set_xticklabels(date_strs[::dt], rotation=45, ha='right')\n",
    "\n",
    "ax.set_ylabel(\"cases\")\n",
    "bx.set_ylabel(\"deaths\", color='red')\n",
    "\n",
    "ax.set_title(region)\n",
    "\n",
    "fig.savefig(f\"../data/out/figures/sars-incidence-data-{region}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Incidence of cases and deaths per day\n",
    "* Weekday-weekend pattern\n",
    "* Delay in death\n",
    "* Increase in cases in July, but not deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some data describing the lockdown\n",
    "[Government response tracker](https://www.bsg.ox.ac.uk/research/research-projects/coronavirus-government-response-tracker)\n",
    "\n",
    "* This data is currently not used: any ideas?\n",
    "* Other useful data: simple measure of testing efforts?\n",
    "\n",
    "**TODO: use new long format file from 0xCGRT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/in/OxCGRT_latest.csv\", 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    ld_table = [row for row in reader]\n",
    "\n",
    "country_code_dict = {\n",
    "    \"United Kingdom\" : \"GBR\",\n",
    "    \"Netherlands\" : \"NLD\",\n",
    "    \"South Africa\" : \"ZAF\",\n",
    "}\n",
    "    \n",
    "country_code = country_code_dict[region]\n",
    "\n",
    "## filter only region-specific data from 0xCGRT\n",
    "sel_ld_table = [rec for rec in ld_table if rec[\"CountryCode\"] == country_code and rec[\"RegionName\"] == '']\n",
    "\n",
    "ld_dates = [datetime.datetime.strptime(rec[\"Date\"], \"%Y%m%d\") for rec in sel_ld_table]\n",
    "ld_ts = [(d - day0).days for d in ld_dates]\n",
    "\n",
    "ld_scores = [rec[\"StringencyIndex\"] for rec in sel_ld_table]\n",
    "ld_scores = [float(x) if x != '' else None for x in ld_scores]\n",
    "\n",
    "## extract testing policy data\n",
    "ld_testpol_codes = [rec[\"H2_Testing policy\"] for rec in sel_ld_table]\n",
    "ld_testpol_codes = [float(x) if x != '' else None for x in ld_testpol_codes]\n",
    "\n",
    "## find periods of testing policies\n",
    "\n",
    "testpol_ts_dict = {\n",
    "    cc : [t for t, c in zip(ld_ts, ld_testpol_codes) if c == cc]\n",
    "    for cc in [0,1,2,3]\n",
    "}\n",
    "\n",
    "testpol_period_dict = {\n",
    "    cc : (np.min(tt), np.max(tt)) for cc, tt in testpol_ts_dict.items()\n",
    "    if len(tt) > 0\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "ax.plot(ts, [row[\"cases_incidence\"] for row in sel_data_dicts], color='k')\n",
    "\n",
    "bx = ax.twinx()\n",
    "bx.plot(ld_ts, ld_scores, color='blue')\n",
    "\n",
    "ax.set_ylabel(\"cases\")\n",
    "bx.set_ylabel(\"stringency index\", color='blue')\n",
    "\n",
    "ax.set_title(region)\n",
    "\n",
    "## color test-policy periods\n",
    "\n",
    "testpol_color_dict = {\n",
    "    0 : 'tab:gray',\n",
    "    1 : 'tab:blue',\n",
    "    2 : 'tab:green',\n",
    "    3 : 'tab:orange'\n",
    "}\n",
    "\n",
    "for c, period in testpol_period_dict.items():\n",
    "    ax.axvspan(*period, color=testpol_color_dict[c], label=str(c), alpha=0.5)\n",
    "\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## export to a simple data file\n",
    "\n",
    "region_nospace = region.replace(\" \", \"_\")\n",
    "with open(f\"../data/in/sars2-{region_nospace}.tsv\", 'w') as f:\n",
    "    for row in sel_data_dicts:\n",
    "        ID = region_nospace\n",
    "        t = row[\"t\"]\n",
    "        ev = \"[]\"\n",
    "        dI = row['cases_incidence']\n",
    "        dIc = defn.uncensored_code\n",
    "        dD = row['deaths_incidence']\n",
    "        dDc = defn.uncensored_code\n",
    "        Nm = row[var_keys[0]]\n",
    "        Nmc = defn.uncensored_code\n",
    "        N = row[total_key]\n",
    "        Nc = defn.uncensored_code\n",
    "        line = f\"{ID}\\t{t}\\t{ev}\\t{dI}\\t{dIc}\\t{dD}\\t{dDc}\\t{Nm}\\t{Nmc}\\t{N}\\t{Nc}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some testing information from OWID database\n",
    "\n",
    "https://github.com/owid/covid-19-data/tree/master/public/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/in/owid-covid-data.json\", 'rb') as f:\n",
    "    owid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_owid_data = owid_data[country_code][\"data\"]\n",
    "owid_dates = [datetime.datetime.strptime(dd[\"date\"], \"%Y-%m-%d\") for dd in sel_owid_data]\n",
    "\n",
    "owid_ts = [(d - day0).days for d in owid_dates]\n",
    "#test_key = \"new_tests\"\n",
    "test_key = \"new_tests_smoothed\"\n",
    "owid_tests = [int(dd[test_key]) if test_key in dd.keys() else None for dd in sel_owid_data]\n",
    "cases_key = \"new_cases\"\n",
    "owid_cases = [int(dd[cases_key]) if cases_key in dd.keys() else None for dd in sel_owid_data]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "ax.plot(owid_ts, owid_tests, color='tab:blue')\n",
    "\n",
    "bx = ax.twinx()\n",
    "bx.plot(owid_ts, owid_cases, color='tab:red')\n",
    "\n",
    "ax.set_ylabel(\"tests\", color='tab:blue')\n",
    "bx.set_ylabel(\"positive tests\", color='tab:red')\n",
    "\n",
    "\n",
    "## add testing policy data\n",
    "for c, period in testpol_period_dict.items():\n",
    "    ax.axvspan(*period, color=testpol_color_dict[c], label=str(c), alpha=0.5)\n",
    "\n",
    "ax.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aggregate case count data to week level \n",
    "to account for periodic reporting pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## define weeks\n",
    "\n",
    "sel_day0 = sel_data_dicts[0][\"date\"]\n",
    "sel_monday0 = sel_day0 - datetime.timedelta(days=sel_day0.weekday())\n",
    "sel_day_last = sel_data_dicts[-1][\"date\"]\n",
    "sel_monday_last = sel_day_last - datetime.timedelta(days=(sel_day_last.weekday()+1)%7)\n",
    "print(sel_monday_last)\n",
    "num_days = (sel_monday_last - sel_monday0).days + 1\n",
    "\n",
    "mondays = [sel_monday0 + datetime.timedelta(days=7*i) for i in range(0, num_days//7 + 1)]\n",
    "\n",
    "## collect incidence between these mondays\n",
    "\n",
    "sel_weekly_data_dicts = []\n",
    "\n",
    "for m0, m1 in zip(mondays[:-1], mondays[1:]):\n",
    "    wdds = [dd for dd in sel_data_dicts if dd[\"date\"] >= m0 and dd[\"date\"] < m1]\n",
    "    C = np.sum([dd[\"cases_incidence\"] for dd in wdds])\n",
    "    D = np.sum([dd[\"deaths_incidence\"] for dd in wdds])\n",
    "    Nmut = np.sum([dd[var_keys[0]] for dd in wdds])\n",
    "    Ntot = np.sum([dd[total_key] for dd in wdds])\n",
    "    sel_weekly_data_dicts.append({\n",
    "        \"date\" : m1,\n",
    "        \"t\" : (m1 - day0).days,\n",
    "        \"cases_incidence\" : C,\n",
    "        \"deaths_incidence\" : D,\n",
    "        var_keys[0] : Nmut,\n",
    "        total_key : Ntot\n",
    "    })\n",
    "    \n",
    "    \n",
    "fig, (ax, cx) = plt.subplots(2, 1, figsize=(14,10), sharex=True)\n",
    "bx = ax.twinx()\n",
    "\n",
    "ws = [row[\"t\"] for row in sel_weekly_data_dicts]\n",
    "\n",
    "ax.plot(ws, [row[\"cases_incidence\"] for row in sel_weekly_data_dicts], color='k', marker='o')\n",
    "bx.plot(ws, [row[\"deaths_incidence\"] for row in sel_weekly_data_dicts], color='red', marker='o')\n",
    "\n",
    "## plot aggregated sequence data\n",
    "\n",
    "Ntots = [row[total_key] for row in sel_weekly_data_dicts]\n",
    "Nmuts = [row[var_keys[0]] for row in sel_weekly_data_dicts]\n",
    "Fms = [x/n if n > 0 else 0.5 for x, n in zip(Nmuts, Ntots)]\n",
    "cx.scatter(ws, Fms, color='k', marker='_')\n",
    "for w, Nmut, Ntot in zip(ws, Nmuts, Ntots):\n",
    "    l, u = sts.beta.interval(0.95, Nmut+0.5, Ntot-Nmut+0.5)\n",
    "    cx.plot([w,w], [l,u], color='k', alpha=0.5)\n",
    "\n",
    "\n",
    "dates = [dd[\"date\"] for dd in sel_weekly_data_dicts]\n",
    "date_strs = [date.strftime(\"%m-%d-%Y\") for date in dates]\n",
    "\n",
    "dt = 2\n",
    "cx.set_xticks(ws[::dt])\n",
    "cx.set_xticklabels(date_strs[::dt], rotation=45, ha='right')\n",
    "\n",
    "ax.set_ylabel(\"cases\")\n",
    "bx.set_ylabel(\"deaths\", color='red')\n",
    "\n",
    "ax.set_title(region)\n",
    "\n",
    "\n",
    "fig.savefig(f\"../data/out/figures/sars-weekly-incidence-data-{region}.pdf\", \n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find date of first mutant case\n",
    "\n",
    "next(filter(lambda dd: dd[var_keys[0]] > 0, sel_weekly_data_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select start and end dates\n",
    "\n",
    "start_date = datetime.datetime.strptime(\"2020-02-25\", \"%Y-%m-%d\") ## NL and UK D614G\n",
    "#start_date = datetime.datetime.strptime(\"2020-09-01\", \"%Y-%m-%d\") ## NL and UK N501Y\n",
    "#start_date = datetime.datetime.strptime(\"2020-09-22\", \"%Y-%m-%d\") ## ZA\n",
    "## WARNING! make sure this starts on a Tuesday!\n",
    "\n",
    "if aa_pos == 614:\n",
    "    end_date = datetime.datetime.strptime(\"2020-08-24\", \"%Y-%m-%d\") ## NL and UK D614G\n",
    "elif aa_pos == 501:\n",
    "    if region == \"United Kingdom\":\n",
    "        end_date = datetime.datetime.strptime(\"2021-02-22\", \"%Y-%m-%d\") ## UK N501Y\n",
    "    elif region == \"Netherlands\":\n",
    "        end_date = datetime.datetime.strptime(\"2021-02-08\", \"%Y-%m-%d\") ## NL N501Y\n",
    "    else:\n",
    "        print(\"WARNING! no end date specified for region \" + region)\n",
    "        end_date = None\n",
    "else:\n",
    "    print(\"WARNING! no end date specified for variant \" + str(aa_pos))\n",
    "    end_date = None\n",
    "    \n",
    "end_date_epi = None\n",
    "\n",
    "if start_date.weekday() != 1:\n",
    "    print(\"WARNING! make sure data stream starts on a Tuesday!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## make a simple file with the data for estavoir\n",
    "date_dict = {dd[\"date\"] : dd for dd in sel_weekly_data_dicts}\n",
    "    \n",
    "with open(f\"../data/in/sars2-weekly-incidence-{region_nospace}-{aa_pos}.tsv\", 'w') as f:\n",
    "    for row in sel_data_dicts:\n",
    "        d = row[\"date\"]\n",
    "        if d < start_date or (end_date is not None and d > end_date):\n",
    "            ## skip dates before a chosen starting point or after chosen end data\n",
    "            continue\n",
    "        if d in date_dict.keys():      \n",
    "            ev = \"[RESET_CASES]\"\n",
    "            wdd = date_dict[d]\n",
    "            if end_date_epi is None or d <= end_date_epi:\n",
    "                #dI, dIc = wdd['cases_incidence'], defn.uncensored_code\n",
    "                dI, dIc = 0, defn.missing_code ## TESTING: don't use cases\n",
    "                dD, dDc = wdd['deaths_incidence'], defn.uncensored_code\n",
    "            else:\n",
    "                dI, dIc = 0, defn.missing_code\n",
    "                dD, dDc = 0, defn.missing_code\n",
    "        else:\n",
    "            ev = \"[]\"\n",
    "            dI, dIc = 0, defn.missing_code\n",
    "            dD, dDc = 0, defn.missing_code\n",
    "        Nm = row[var_keys[0]]\n",
    "        Nmc = defn.uncensored_code\n",
    "        N = row[total_key]\n",
    "        Nc = defn.uncensored_code\n",
    "        ID = region_nospace\n",
    "        t = row[\"t\"]\n",
    "        line = f\"{ID}\\t{t}\\t{ev}\\t{dI}\\t{dIc}\\t{dD}\\t{dDc}\\t{Nm}\\t{Nmc}\\t{N}\\t{Nc}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a simple file with the data for estavoir\n",
    "\n",
    "date_dict = {dd[\"date\"] : dd for dd in sel_weekly_data_dicts}\n",
    "    \n",
    "with open(f\"../data/in/sars2-seq-death-week-{region_nospace}-{aa_pos}.tsv\", 'w') as f:\n",
    "    for row in sel_weekly_data_dicts:\n",
    "        d = row[\"date\"]\n",
    "        if d < start_date or (end_date is not None and d > end_date):\n",
    "            ## skip dates before a chosen starting point or after chosen end data\n",
    "            continue\n",
    "        ev = \"[RESET_CASES]\"\n",
    "        if end_date_epi is None or d <= end_date_epi:\n",
    "            dD, dDc = row['deaths_incidence'], defn.uncensored_code\n",
    "            dI, dIc = row['cases_incidence'], defn.uncensored_code\n",
    "        else:\n",
    "            dD, dDc = 0, defn.missing_code\n",
    "            dI, dIc = 0, defn.missing_code\n",
    "        Nm = row[var_keys[0]]\n",
    "        Nmc = defn.uncensored_code\n",
    "        N = row[total_key]\n",
    "        Nc = defn.uncensored_code\n",
    "        ID = region_nospace\n",
    "        t = row[\"t\"]\n",
    "        line = f\"{ID}\\t{t}\\t{ev}\\t{dI}\\t{dIc}\\t{dD}\\t{dDc}\\t{Nm}\\t{Nmc}\\t{N}\\t{Nc}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the day number of border closing\n",
    "\n",
    "date_close = datetime.datetime.strptime(\"12-21-2020\", \"%m-%d-%Y\")\n",
    "\n",
    "print(\"day number border closing:\", (date_close-day0).days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Plot the results from SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#pfout_file = \"../data/out/ipf_result-sars_model.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/wk-seq/NL/D614G/ipf_result-sars_model_NL-614-wk.xml\"\n",
    "#pfout_file = \"../data/out/wk-seq/UK/D614G/ipf_result-sars_model_UK-614-wk.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/wk-seq/UK/B117/ipf_result-sars_model_UK-501-wk.xml\"\n",
    "#pfout_file = \"../data/out/wk-seq/NL/B117/ipf_result-sars_model_NL-501-wk.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/wk-seq/NL/B117/ipf_result-sars_model_NL-501-wk_sigma=0.7.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/wk-seq/UK/B117/ipf_result-sars_model_UK-501-long-wk.xml\"\n",
    "pfout_file = \"../data/out/wk-seq/NL/B117/ipf_result-sars_model_NL-501-long-wk.xml\"\n",
    "\n",
    "\n",
    "#pfout_file = \"../data/out/wk-seq/UK/D614G/ipf_result-sars_model_UK-614-wk-long.xml\"\n",
    "#pfout_file = \"../data/out/wk-seq/NL/D614G/ipf_result-sars_model_NL-614-wk-long.xml\"\n",
    "\n",
    "\n",
    "### OLD FILES\n",
    "\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model_UK-501_10K.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model_NL-501_10K.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model_NL-614_10K_short.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model_UK-614_10K_short.xml\"\n",
    "\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-NL-fit_sigma_od.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-UK-fit_sigma_od.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-fit_sigma_od_r-UK.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-fit_sigma_od_r-NL.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-NLnu.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model_sigma=0.36.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-migration.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-UK_N501Y_10K.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-UK_D614G_10K.xml\"\n",
    "#pfout_file = \"../data/out/ipf_result-sars_model-NL_614_10K.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "idx = -1 ## select one of the PF iterations\n",
    "\n",
    "#pf_data = pftools.extract_pfilter_data(pfout_file, idx=idx, parnames=parnames)\n",
    "pf_data = pftools.extract_pfilter_data(pfout_file)\n",
    "parnames = pf_data[\"parnames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fit_dicts = {}  \n",
    "\n",
    "for r, ID in enumerate(pf_data[\"pfIDs\"]):\n",
    "    ## make a dictionary\n",
    "    fit_dicts[ID] = {\n",
    "        \"params\" : pftools.makeParDict(r, parnames, pf_data[\"param_medians\"], \n",
    "                                       pf_data[\"param_ranges\"])\n",
    "        ## TODO: other elements?\n",
    "    }\n",
    "    \n",
    "## TODO: extract hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "stat_dicts = {}\n",
    "\n",
    "for r, ID in enumerate(pf_data[\"pfIDs\"]):\n",
    "    ts = [float(pf.attrib[\"t\"]) for pf in pf_data[\"particle_filters\"][ID]]\n",
    "    Jeffs = [int(pf.attrib[\"J_eff\"]) for pf in pf_data[\"particle_filters\"][ID]]\n",
    "    Jcofs = [int(pf.attrib[\"J_coffin\"]) for pf in pf_data[\"particle_filters\"][ID]]\n",
    "    Jsims = [float(pf.attrib[\"J_inv_simpson\"]) for pf in pf_data[\"particle_filters\"][ID]]\n",
    "    cLLs = [float(pf.attrib[\"cond_LL_hat\"]) for pf in pf_data[\"particle_filters\"][ID]]\n",
    "    stat_dicts[ID] = {\n",
    "        \"ts\" : ts,\n",
    "        \"Jeffs\" : Jeffs,\n",
    "        \"Jsims\" : Jsims,\n",
    "        \"Jcofs\" : Jcofs,\n",
    "        \"cLLs\" : cLLs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Figure for manuscript\n",
    "\n",
    "## OPTIONS\n",
    "\n",
    "INCL_INSET = False\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(10,12), sharex=True)\n",
    "\n",
    "ID = pf_data[\"pfIDs\"][0] ## select single ID\n",
    "\n",
    "###### TRAJECTORIES OF LATENT VARIABLES ######\n",
    "\n",
    "## plot trajectories on a linear scale\n",
    "\n",
    "varnames = [\"Iw\", \"Im\", \"H\"]\n",
    "pretty_varnames = [\"$I_w$\", \"$I_m$\", \"$H$\"]\n",
    "\n",
    "trajcolors = ['tab:orange', 'tab:blue', 'tab:red']\n",
    "\n",
    "alpha_traj = 0.7\n",
    "\n",
    "ax = axs[0]\n",
    "if INCL_INSET:\n",
    "    axins = inset_axes(ax, width=\"12%\", height=\"30%\", loc=2)\n",
    "## PLOT FILTERED PATHS\n",
    "for j, path in enumerate(pf_data[\"paths\"][ID]):\n",
    "    ## extract timeseries\n",
    "    xs = path.findall(\"state\")\n",
    "    ts = [float(x.attrib[\"t\"]) for x in xs]\n",
    "    for color, X, lab in zip(trajcolors, varnames, pretty_varnames):\n",
    "        Xs = [float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"]) for x in xs]\n",
    "        ## plot\n",
    "        kwargs = {\"label\" : lab} if j == 0 else {}\n",
    "        ax.plot(ts, Xs, color=color, alpha=alpha_traj, linewidth=0.5, zorder=1, **kwargs)\n",
    "        if INCL_INSET:\n",
    "            axins.plot(ts, Xs, color=color, alpha=alpha_traj, linewidth=0.5, zorder=1, **kwargs)\n",
    "            ## restricy limits of axins\n",
    "            axins.set_xlim(ts[0], 75)\n",
    "            axins.set_ylim(-50, 2e3)\n",
    "            axins.yaxis.set_label_position(\"right\")\n",
    "            axins.yaxis.tick_right()\n",
    "            axins.tick_params(axis='both', which='major', labelsize='x-small')\n",
    "## labels\n",
    "ax.set_ylabel(\"population size\")\n",
    "leg = ax.legend(ncol=1, loc=2, fontsize='small')\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    lh.set_linewidth(1)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(evplot.y_fmt))\n",
    "\n",
    "\n",
    "###### DATA AND PREDICTIONS ######\n",
    "\n",
    "trajcolor = [\"pink\", \"deepskyblue\"]\n",
    "varcolor = ['purple', 'tab:blue']\n",
    "obsvarnames = ['D', 'Fm']\n",
    "data_markers = ['o', '_']\n",
    "#legend_locs = [1, 4]\n",
    "#legend_locs = [1, 1]\n",
    "legend_locs = [2, 2]\n",
    "data_colors = ['w', 'lightgray']\n",
    "obslabels = [\"deaths\", \"mutant frequency\"]\n",
    "yscales = ['linear', 'linear']\n",
    "alpha=0.7\n",
    "dt = 1\n",
    "\n",
    "Ob = len(obsvarnames)\n",
    "\n",
    "\n",
    "print(\"sampled\", len(pf_data[\"paths\"][ID]), \"trajectories for\", ID)\n",
    "## PLOT DATA\n",
    "# deaths\n",
    "ax = axs[1]\n",
    "ws = [row[\"t\"] for row in sel_weekly_data_dicts]\n",
    "Ds = [row[\"deaths_incidence\"] for row in sel_weekly_data_dicts]\n",
    "ax.scatter(ws, Ds, color='k', edgecolor='k', zorder=4, label='data', s=20)    \n",
    "# mutant freq\n",
    "ax = axs[2]\n",
    "ts = [row[\"t\"] for row in sel_data_dicts if row[total_key] > 0]\n",
    "Fms = [row[var_keys[0]] / n for row in sel_data_dicts if (n := row[total_key]) > 0]\n",
    "## CIs\n",
    "lFms = [sts.beta.ppf(0.025, row[var_keys[0]]+0.5, n - row[var_keys[0]]+0.5) \n",
    "        for row in sel_data_dicts if (n := row[total_key]) > 0]\n",
    "uFms = [sts.beta.ppf(0.975, row[var_keys[0]]+0.5, n - row[var_keys[0]]+0.5) \n",
    "        for row in sel_data_dicts if (n := row[total_key]) > 0]\n",
    "for t, l, u in zip(ts, lFms, uFms):\n",
    "    ax.plot([t,t], [l,u], color='k', alpha=0.3)\n",
    "ax.scatter(ts, Fms, color='k', edgecolor='k', zorder=4, label='data', s=20,  marker='|')\n",
    "## PLOT FILTERED PATHS\n",
    "for path in pf_data[\"paths\"][ID]:\n",
    "    for i, X in enumerate(obsvarnames):\n",
    "        ## extract timeseries\n",
    "        xs = path.findall(\"state\")\n",
    "        ts = [float(x.attrib[\"t\"]) for x in xs]\n",
    "        Xs = [float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"]) for x in xs]\n",
    "        ## plot\n",
    "        ax = axs[i+1]\n",
    "        ax.plot(ts, Xs, color=trajcolor[i], alpha=alpha, linewidth=0.5, zorder=1)\n",
    "## PLOT PREDICTION RANGES\n",
    "ts = [float(x.attrib[\"t\"]) for x in pf_data[\"pred_medians\"][ID]]\n",
    "for i, X in enumerate(obsvarnames):\n",
    "    if X in [\"C\", \"D\", 'Fm']:\n",
    "        ws = [row[\"t\"] for row in sel_weekly_data_dicts]\n",
    "        mask = [False if t in ws else True for t in ts]\n",
    "    else:\n",
    "        mask = None\n",
    "    ax = axs[i+1]\n",
    "    rans = pf_data[\"ranges\"][ID]\n",
    "    Xs_ran = [[float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"]) for x in ran]\n",
    "              for ran in rans]\n",
    "    Xs_pred = [float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"])\n",
    "               for x in pf_data[\"pred_medians\"][ID]]\n",
    "    Xs_filt = [float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"])\n",
    "               for x in pf_data[\"filter_medians\"][ID]]\n",
    "    evplot.pfilter_boxplot(ax, ts, Xs_ran, Xs_pred, Xs_filt, mask=mask,\n",
    "                           color=varcolor[i], dt=dt)\n",
    "## lims and labels\n",
    "for i, X in enumerate(obsvarnames):\n",
    "    ax = axs[i+1]\n",
    "    #ax.set_ylim(0, ylims[i])\n",
    "    ax.set_yscale(yscales[i])\n",
    "    ## labels\n",
    "    ax.set_ylabel(obslabels[i])\n",
    "    ## Legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker=data_markers[i], color=data_colors[i], label='data', \n",
    "               markerfacecolor='k', markeredgecolor='k', markersize=7),\n",
    "        Line2D([0], [0], color=varcolor[i], label='model'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, ncol=1, fontsize='small', loc=legend_locs[i])\n",
    "    \n",
    "axs[2].set_ylim(-0.05, 1.05)\n",
    "axs[0].set_title(region)\n",
    "axs[-1].set_xlabel(f\"days since {day0.strftime('%m-%d-%Y')}\")\n",
    "\n",
    "dx_num = -0.15\n",
    "\n",
    "axs[0].text(dx_num, 1.0, \"A\", fontsize=24, transform=axs[0].transAxes)\n",
    "axs[1].text(dx_num, 1.0, \"B\", fontsize=24, transform=axs[1].transAxes)\n",
    "axs[2].text(dx_num, 1.0, \"C\", fontsize=24, transform=axs[2].transAxes)\n",
    "\n",
    "#axs[1].set_ylim(0, 400)\n",
    "\n",
    "#axs[0].set_yscale(\"log\")\n",
    "\n",
    "tmin = (start_date - day0).days-3\n",
    "#tmax = 250\n",
    "tmax = 430\n",
    "#tmax = 360\n",
    "axs[-1].set_xlim(tmin, tmax)\n",
    "   \n",
    "    \n",
    "fig.align_ylabels(axs)\n",
    "    \n",
    "fig.savefig(f\"../data/out/figures/pf-traj-fit-sars_model-{region_nospace}.pdf\", \n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## plot PF statistics\n",
    "\n",
    "R = len(pf_data[\"pfIDs\"])\n",
    "\n",
    "fig, axs = plt.subplots(R, 1, figsize=(10,3*R))\n",
    "\n",
    "if R == 1:\n",
    "    axs = np.array([axs])\n",
    "\n",
    "bxs = [ax.twinx() for ax in axs]\n",
    "\n",
    "for i, ID in enumerate(pf_data[\"pfIDs\"]):\n",
    "    stat_dict = stat_dicts[ID]\n",
    "    ax = axs[i]\n",
    "    ax.plot(stat_dict[\"ts\"], stat_dict[\"cLLs\"], color='k')\n",
    "    ## plot PF statistics\n",
    "    bx = bxs[i]\n",
    "    bx.plot(stat_dict[\"ts\"], stat_dict[\"Jeffs\"], color='tab:red', alpha=0.7)\n",
    "    #bx.plot(stat_dict[\"ts\"], stat_dict[\"Jsims\"], color='tab:purple')\n",
    "    ax.set_ylabel(\"conditional\\nlog-likelihood\")\n",
    "    bx.set_ylabel(\"effective\\nswarm size\", color='red')\n",
    "    \n",
    "## share axes after the fact\n",
    "bxs[0].get_shared_y_axes().join(*bxs)\n",
    "bxs[0].autoscale(axis='y')\n",
    "    \n",
    "\n",
    "fig.savefig(\"../data/out/figures/pf-stats-panel-sars_model.pdf\",\n",
    "            bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SMC Diagnostics\n",
    "Conditional likelihood and effective number of particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sel_parnames = parnames\n",
    "\n",
    "fig, axs = plt.subplots(len(sel_parnames)+1, 1, \n",
    "                        figsize=(14,2*len(sel_parnames)), \n",
    "                        sharex=True)\n",
    "\n",
    "## FIXME: plot a single run for the evolution of a parameter\n",
    "\n",
    "for ID in pf_data[\"pfIDs\"]:\n",
    "    paramss = fit_dicts[ID][\"params\"]\n",
    "    for i, pn in enumerate(sel_parnames):\n",
    "        ax = axs[i]\n",
    "        meds = paramss[pn][\"median\"]\n",
    "        rans = paramss[pn][\"range\"]\n",
    "        ms = range(len(meds))\n",
    "        evplot.range_plots(ax, ms, *aux.unzip(rans), dt=0.1, zorder=1)\n",
    "        ax.scatter(ms, meds, color='r', marker='_', zorder=2) \n",
    "        ax.set_ylabel(pn)\n",
    "    \n",
    "    \n",
    "## plot log-likelihood\n",
    "\n",
    "ll_dicts = [xs.find(\"log_lik\").attrib for xs in pf_data[\"iterf_steps\"]]\n",
    "ll_vals = [float(d[\"val\"]) for d in ll_dicts]\n",
    "ll_colors = ['k' if d[\"finite\"] == 'true' else 'red' for d in ll_dicts]\n",
    "ms = range(len(ll_dicts))\n",
    "axs[-1].scatter(ms, ll_vals, color=ll_colors)\n",
    "axs[-1].set_ylabel(\"LL\")\n",
    "axs[-1].set_xlabel(\"iteration\")\n",
    "#axs[-1].set_ylim(-500, -300)\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "if len(ms) > 2:\n",
    "    lb = -np.infty\n",
    "    ## filter out some mistakes\n",
    "    fms = [m for ll, m in zip(ll_vals, ms) if ll > lb]\n",
    "    flls = [ll for ll, m in zip(ll_vals, ms) if ll > lb]\n",
    "    cs = UnivariateSpline(fms, flls, s=1e4)\n",
    "    xs = np.linspace(ms[0], ms[-1], 1000)\n",
    "    axs[-1].plot(xs, cs(xs), label='spline', color='red', linewidth=2)\n",
    "\n",
    "print(\"final LL:\", ll_vals[-1])\n",
    "\n",
    "fig.savefig(\"../data/out/figures/traces-panel-sars_model.pdf\", \n",
    "            bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Print some parameter estimates...\n",
    "\n",
    "par_ests = {\n",
    "    ID: {\n",
    "        k : v[\"median\"][-1]\n",
    "        for k, v in fit_dicts[ID][\"params\"].items()\n",
    "    } \n",
    "    for ID in pf_data[\"pfIDs\"]\n",
    "}\n",
    "\n",
    "for ID in par_ests.keys():\n",
    "    for k, v in par_ests[ID].items():\n",
    "        print(f\"{ID} -> {k}: {v:0.3g}\")\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get estimate of absolute prevalence of mutant at a particular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprevs = [] ## to be filled below...\n",
    "\n",
    "target_time = (date_close-day0).days\n",
    "\n",
    "for path in pf_data[\"paths\"][ID]:\n",
    "    ## extract timeseries\n",
    "    xs = path.findall(\"state\")\n",
    "    ts = np.array([float(x.attrib[\"t\"]) for x in xs])\n",
    "    idx = np.argmin(np.abs(ts - target_time))\n",
    "    Im = float(xs[idx].find(f\"var_vec[@name='Im']/var\").attrib[\"val\"])\n",
    "    Em = float(xs[idx].find(f\"var_vec[@name='Em']/var\").attrib[\"val\"])\n",
    "    fprevs.append(Im + Em)\n",
    "\n",
    "print(np.mean(fprevs))\n",
    "np.percentile(fprevs, [0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate cases and mutant frequency with piecewise linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get mutant frequency data\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "fpaths = [] ## to be filled below...\n",
    "ftimes = [float(x.attrib[\"t\"]) for x in pf_data[\"paths\"][ID][0].findall(\"state\")]\n",
    "\n",
    "for path in pf_data[\"paths\"][ID]:\n",
    "    ## extract timeseries\n",
    "    xs = path.findall(\"state\")\n",
    "    ts = [float(x.attrib[\"t\"]) for x in xs]\n",
    "    Xs = [float(x.find(f\"var_vec[@name='Fm']/var\").attrib[\"val\"]) for x in xs]\n",
    "    fpaths.append(Xs)\n",
    "    ## plot trajectories\n",
    "    ax.plot(ts, Xs, color='tab:blue', alpha=0.2)\n",
    "    \n",
    "mean_fpath = np.mean(fpaths, axis=0)\n",
    "ax.plot(ftimes, mean_fpath, color='k')\n",
    "\n",
    "## reduce number of points\n",
    "\n",
    "print(\"number of time points:\", len(ftimes))\n",
    "\n",
    "stride = 15\n",
    "\n",
    "ftimes_red = ftimes[::stride] + [ftimes[-1]]\n",
    "mean_fpath_red = list(mean_fpath[::stride]) + [mean_fpath[-1]]\n",
    "\n",
    "ax.plot(ftimes_red, mean_fpath_red, color='red')\n",
    "\n",
    "print(\"number of break points:\", len(ftimes_red))\n",
    "\n",
    "## write values to file\n",
    "\n",
    "with open(\"../Fm_lin_fun.txt\", 'w') as f:\n",
    "    f.write(' '.join(map(str, ftimes_red)) + '\\n')\n",
    "    f.write(' '.join(map(str, mean_fpath_red)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get relative prevalence data\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "fpaths = [] ## to be filled below...\n",
    "ftimes = [float(x.attrib[\"t\"]) for x in pf_data[\"paths\"][ID][0].findall(\"state\")]\n",
    "\n",
    "for path in pf_data[\"paths\"][ID]:\n",
    "    ## extract timeseries\n",
    "    xs = path.findall(\"state\")\n",
    "    ts = [float(x.attrib[\"t\"]) for x in xs]\n",
    "    varnames = [\"S\", \"I\", \"E\", \"H\", \"R\"]\n",
    "    Xss = [[float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"]) for x in xs]\n",
    "           for X in varnames]\n",
    "    Ns = np.sum(Xss, axis=0)\n",
    "    Is = Xss[1]\n",
    "    relIs = [I/N for I, N in zip(Is, Ns)]\n",
    "    fpaths.append(relIs)\n",
    "    ## plot trajectories\n",
    "    ax.plot(ts, relIs, color='tab:blue', alpha=0.2)\n",
    "    \n",
    "mean_fpath = np.mean(fpaths, axis=0)\n",
    "ax.plot(ftimes, mean_fpath, color='k')\n",
    "\n",
    "## reduce number of points\n",
    "\n",
    "print(\"number of time points:\", len(ftimes))\n",
    "\n",
    "stride = 15\n",
    "\n",
    "ftimes_red = ftimes[::stride] + [ftimes[-1]]\n",
    "mean_fpath_red = list(mean_fpath[::stride]) + [mean_fpath[-1]]\n",
    "\n",
    "ax.plot(ftimes_red, mean_fpath_red, color='red')\n",
    "\n",
    "print(\"number of break points:\", len(ftimes_red))\n",
    "\n",
    "## write values to file\n",
    "\n",
    "with open(\"../I_lin_fun.txt\", 'w') as f:\n",
    "    f.write(' '.join(map(str, ftimes_red)) + '\\n')\n",
    "    f.write(' '.join(map(str, mean_fpath_red)) + '\\n')\n",
    "    \n",
    "    \n",
    "## TODO: write both timeseries to one file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $R_e$ the effective reproduction number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_Re(beta, gamma, nu, s, Fm, S, N):\n",
    "    return beta/(gamma+nu) * S/N * ((1-Fm) + Fm*(1+s))\n",
    "\n",
    "def Hv(t, u):\n",
    "    return scipy.special.expit(t/u)\n",
    "\n",
    "def betat(t, bvec, tvec, uvec):\n",
    "    ls = [1] + [Hv(t-ti, uvec[i]) for i, ti in enumerate(tvec)]\n",
    "    rs = [(1-Hv(t-ti, uvec[i])) for i, ti in enumerate(tvec)] + [1]\n",
    "    return np.sum([b*l*r for b, l, r in zip(bvec, ls, rs)])\n",
    "\n",
    "\n",
    "## define parameters\n",
    "ID = list(par_ests.keys())[0]\n",
    "\n",
    "num_breakpoints = 3\n",
    "\n",
    "bvec = [par_ests[ID][f\"beta{i}\"] for i in range(num_breakpoints+1)]\n",
    "tvec = [par_ests[ID][f\"t{i}\"] for i in range(1,num_breakpoints+1)]\n",
    "s = par_ests[ID][\"sigma\"]\n",
    "## FIXED parameters\n",
    "uvec = [1.189, 1.189, 1.189]\n",
    "gamma = 0.25\n",
    "nu = 0.005\n",
    "\n",
    "fig, (bx, ax) = plt.subplots(2, 1, figsize=(7,5), sharex=True)\n",
    "\n",
    "fpaths_av = [] ## to be filled below...\n",
    "fpaths = [] ## to be filled below...\n",
    "ftimes = [float(x.attrib[\"t\"]) for x in pf_data[\"paths\"][ID][0].findall(\"state\")]\n",
    "\n",
    "for path in pf_data[\"paths\"][ID]:\n",
    "    ## extract timeseries\n",
    "    xs = path.findall(\"state\")\n",
    "    ts = [float(x.attrib[\"t\"]) for x in xs]\n",
    "    varnames = [\"S\", \"Ew\", \"Em\", \"Iw\", \"Im\", \"H\", \"R\"]\n",
    "    Xss = [[float(x.find(f\"var_vec[@name='{X}']/var\").attrib[\"val\"]) for x in xs]\n",
    "           for X in varnames]\n",
    "    Ns = np.sum(Xss, axis=0)\n",
    "    Iws, Ims = np.array(Xss[3]), np.array(Xss[4]) ## TODO: add Em and Ew to the equation?\n",
    "    Fms = Ims / (Iws + Ims)\n",
    "    Ss = Xss[0]\n",
    "    bts = [betat(t, bvec, tvec, uvec) for t in ts]\n",
    "    avRe = [average_Re(bt, gamma, nu, s, Fm, S, N) for (S, Fm, N, bt) in zip(Ss, Fms, Ns, bts)]\n",
    "    Re = [average_Re(bt, gamma, nu, 0, Fm, S, N) for (S, Fm, N, bt) in zip(Ss, Fms, Ns, bts)]\n",
    "    fpaths.append(Re)\n",
    "    fpaths_av.append(avRe)\n",
    "    ## plot trajectories\n",
    "    \n",
    "mean_fpath = np.mean(fpaths, axis=0)\n",
    "ci_fpath = np.percentile(fpaths, axis=0, q=[2.5,97.5])\n",
    "ax.plot(ftimes, mean_fpath, color='k')\n",
    "ax.fill_between(ftimes, *ci_fpath, linewidth=0, color='k', alpha=0.3)\n",
    "\n",
    "mean_fpath_av = np.mean(fpaths_av, axis=0)\n",
    "ci_fpath_av = np.percentile(fpaths_av, axis=0, q=[2.5,97.5])\n",
    "ax.plot(ftimes, mean_fpath_av, color='r')\n",
    "ax.fill_between(ftimes, *ci_fpath_av, linewidth=0, color='r', alpha=0.3)\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "## add dates to x-axis\n",
    "\n",
    "xmin, xmax = int(np.min(ftimes)), int(np.max(ftimes))\n",
    "\n",
    "xmin += 7 ## correction...\n",
    "dt = 14\n",
    "\n",
    "xticks = [xmin + dt * i for i in range(0,(xmax-xmin)//dt+2)]\n",
    "\n",
    "xdates = [day0 + datetime.timedelta(days=d) for d in xticks]\n",
    "xticklabels = [d.strftime(\"%b %d\") for d in xdates]\n",
    "\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xticklabels, fontsize='x-small', rotation=45, ha='right')\n",
    "\n",
    "ax.set_ylabel(\"$R_e$\")\n",
    "ax.set_xlabel(\"date\")\n",
    "\n",
    "## plot the time-varying infection rate beta(t)\n",
    "\n",
    "betats = [betat(t, bvec, tvec, uvec) for t in ftimes]\n",
    "bx.plot(ftimes, betats, color='k', linewidth=2, zorder=2)\n",
    "\n",
    "def hybrid_trans_x(ax):\n",
    "    return blended_transform_factory(ax.transData, ax.transAxes)\n",
    "\n",
    "def hybrid_trans_y(ax):\n",
    "    return blended_transform_factory(ax.transAxes, ax.transData)\n",
    "\n",
    "W = 7\n",
    "\n",
    "for i, t in enumerate(tvec):\n",
    "    bx.axvspan(t-W/2, t+W/2, zorder=1, color='k', alpha=0.3, linewidth=0)\n",
    "    bx.text(t, 1, f\"$t_{i+1}$\", transform=hybrid_trans_x(bx), ha='center', va='bottom')\n",
    "\n",
    "for i, b in enumerate(bvec):\n",
    "    bx.text(1.01, b, f\"$\\\\beta_{i}$\", transform=hybrid_trans_y(bx), va='center', ha='left')\n",
    "    if i < len(tvec):\n",
    "        xmin = tvec[i]-W if i < len(tvec) else max(ftimes)\n",
    "        xmax = max(ftimes)\n",
    "        bx.plot([xmin, xmax], [b,b], linestyle='--', linewidth=0.5, color='k', zorder=2)\n",
    "    \n",
    "bx.set_ylabel(\"$\\\\beta$\")\n",
    "bx.set_ylim(-0.025, 0.425)\n",
    "\n",
    "dx_num = -0.15\n",
    "\n",
    "ax.text(dx_num, 1.0, \"B\", fontsize=24, transform=ax.transAxes)\n",
    "bx.text(dx_num, 1.0, \"A\", fontsize=24, transform=bx.transAxes)\n",
    "\n",
    "fig.savefig(\"../data/out/figures/B117_UK_Re.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
